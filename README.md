# Visual Question Answering (VQA) Project

A project to explore, implement, and understand models for the task of Visual Question Answering (VQA). The goal is to build a system that can answer natural language questions about images.

## ðŸŽ¯ Project Goals

- **Literature Review:** Survey foundational and state-of-the-art papers in VQA (e.g., Show-Ask-Attend-Answer, ViLT, BLIP).
- **Baseline Model:** Implement a basic model (CNN for image features + LSTM/Transformer for text) to establish a performance baseline.
- **Leverage Pre-trained Models:** Experiment with vision and language models from Hugging Face (e.g., ViT, BERT, CLIP).
- **Benchmarking:** Train and evaluate the model on the **VQA v2.0** dataset.
- **Documentation:** Maintain a clean codebase and detailed learning log.

## ðŸ› ï¸ Tech Stack

- **Frameworks:** `PyTorch` (Primary) / `TensorFlow`
- **Libraries:** `Hugging Face Transformers`, `TorchVision`, `OpenCV`, `Pandas`, `NumPy`
- **Tools:** `Jupyter Notebook`, `Git`

## ðŸ“‚ Project Structure
visual-question-answering/
â”‚
â”œâ”€â”€ data/ # Dataset files (not versioned on Git)
â”œâ”€â”€ notebooks/ # For exploratory analysis and prototyping
â”œâ”€â”€ research/ # Research papers, notes, and project planning
â”œâ”€â”€ src/ # Source code for main scripts and modules
â”œâ”€â”€ .gitignore # Files and folders to ignore
â”œâ”€â”€ LICENSE # MIT License
â””â”€â”€ README.md # This file


## ðŸ“… Progress Log

### Week 1: Project Initiation & Literature Review
- [x] Initialize project repository with structure.
- [ ] Research and summarize key VQA papers.
- [ ] Set up Python environment and core dependencies.
- [ ] Perform initial data exploration.

---

> **Note:** This project is under active development as part of a focused learning sprint.
