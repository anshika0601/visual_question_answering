# Visual Question Answering (VQA) Project

A project to explore, implement, and understand models for the task of Visual Question Answering (VQA). The goal is to build a system that can answer natural language questions about images.

## 🎯 Project Goals

- **Literature Review:** Survey foundational and state-of-the-art papers in VQA (e.g., Show-Ask-Attend-Answer, ViLT, BLIP).
- **Baseline Model:** Implement a basic model (CNN for image features + LSTM/Transformer for text) to establish a performance baseline.
- **Leverage Pre-trained Models:** Experiment with vision and language models from Hugging Face (e.g., ViT, BERT, CLIP).
- **Benchmarking:** Train and evaluate the model on the **VQA v2.0** dataset.
- **Documentation:** Maintain a clean codebase and detailed learning log.

## 🛠️ Tech Stack

- **Frameworks:** `PyTorch` (Primary) / `TensorFlow`
- **Libraries:** `Hugging Face Transformers`, `TorchVision`, `OpenCV`, `Pandas`, `NumPy`
- **Tools:** `Jupyter Notebook`, `Git`

## 📂 Project Structure
visual-question-answering/
│
├── data/ # Dataset files (not versioned on Git)
├── notebooks/ # For exploratory analysis and prototyping
├── research/ # Research papers, notes, and project planning
├── src/ # Source code for main scripts and modules
├── .gitignore # Files and folders to ignore
├── LICENSE # MIT License
└── README.md # This file


## 📅 Progress Log

### Week 1: Project Initiation & Literature Review
- [x] Initialize project repository with structure.
- [ ] Research and summarize key VQA papers.
- [ ] Set up Python environment and core dependencies.
- [ ] Perform initial data exploration.

---

> **Note:** This project is under active development as part of a focused learning sprint.
